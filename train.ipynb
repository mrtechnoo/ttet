{"cells":[{"cell_type":"code","execution_count":null,"id":"f8141741","metadata":{"id":"f8141741"},"outputs":[],"source":["import os\n","import json\n","import time\n","import argparse\n","import random\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"id":"68d3b8cc","metadata":{"id":"68d3b8cc","outputId":"6454ca7a-b453-497e-f47a-aada7baebea6"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.models as models\n","from torchvision import datasets"]},{"cell_type":"code","execution_count":null,"id":"d62f7b47","metadata":{"id":"d62f7b47","outputId":"20deead0-3f78-4e3b-cdeb-4dbedaf9ccb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n","Collecting wandb\n","  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 51.9 MB/s eta 0:00:01\n","\u001b[?25hCollecting promise<3,>=2.0\n","  Downloading promise-2.3.tar.gz (19 kB)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from wandb) (6.0)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (8.0.4)\n","Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.27.1)\n","Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.16.0)\n","Collecting setproctitle\n","  Downloading setproctitle-1.2.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.8.0-py2.py3-none-any.whl (153 kB)\n","\u001b[K     |████████████████████████████████| 153 kB 63.9 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from wandb) (59.5.0)\n","Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.9.1)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.19.4)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 128.3 MB/s eta 0:00:01\n","\u001b[?25hCollecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 89.0 MB/s  eta 0:00:01\n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.9)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2022.5.18.1)\n","Building wheels for collected packages: promise, pathtools\n","  Building wheel for promise (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=d459d8b2ca3642a065933c641f805e33a590ce08f9aee274e1fc0bfddf5b55c9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0juu_29p/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n","  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=bd65fc4db45f2759bbd2a01d93e6df6ddccd871d60772d4437bd2d5fc492cdf5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0juu_29p/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n","Successfully built promise pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, promise, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 promise-2.3 sentry-sdk-1.8.0 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 wandb-0.12.21\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["!pip install wandb"]},{"cell_type":"code","execution_count":null,"id":"946e4987","metadata":{"id":"946e4987"},"outputs":[],"source":["# Mixed Precision with Apex and Monitoring with Wandb\n","import wandb\n","from apex import amp\n","from apex.optimizers import FusedAdam"]},{"cell_type":"code","execution_count":null,"id":"a5bd6ee2","metadata":{"id":"a5bd6ee2"},"outputs":[],"source":["# FOR DISTRIBUTED: (can also use torch.nn.parallel.DistributedDataParallel instead)\n","from apex.parallel import DistributedDataParallel"]},{"cell_type":"code","execution_count":null,"id":"888b4593","metadata":{"id":"888b4593","outputId":"f487b09a-e4e2-4323-8093-d35cab83fa8e"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdin","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"code","execution_count":null,"id":"a50bbcd1","metadata":{"id":"a50bbcd1"},"outputs":[],"source":["#GPU using CUDA\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","try:\n","    os.makedirs(\"./saved\")\n","except FileExistsError:\n","    # directory already exists\n","    pass"]},{"cell_type":"code","execution_count":null,"id":"b3440965","metadata":{"id":"b3440965","outputId":"2a492de4-a3d7-44e6-d176-0191c6a919ab"},"outputs":[{"name":"stderr","output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--local_rank LOCAL_RANK]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-e3fe5e3a-514c-4b6f-870e-59b6f97ed728.json\n"]},{"ename":"SystemExit","evalue":"2","output_type":"error","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]}],"source":["parser = argparse.ArgumentParser()\n","# FOR DISTRIBUTED:  Parse for the local_rank argument, which will be supplied\n","# automatically by torch.distributed.launch.\n","parser.add_argument(\"--local_rank\", default=0, type=int)\n","args = parser.parse_args()\n"]},{"cell_type":"code","execution_count":null,"id":"ecef1ab3","metadata":{"id":"ecef1ab3","outputId":"063c0dae-1f96-4ada-e625-40d9ee1e1e7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n","Collecting opendatasets\n","  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n","Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from opendatasets) (8.0.4)\n","Collecting kaggle\n","  Downloading kaggle-1.5.12.tar.gz (58 kB)\n","\u001b[K     |████████████████████████████████| 58 kB 47.6 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from opendatasets) (4.64.0)\n","Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.8/site-packages (from kaggle->opendatasets) (1.16.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from kaggle->opendatasets) (2022.5.18.1)\n","Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.8/site-packages (from kaggle->opendatasets) (2.8.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from kaggle->opendatasets) (2.27.1)\n","Requirement already satisfied: python-slugify in /opt/conda/lib/python3.8/site-packages (from kaggle->opendatasets) (6.1.2)\n","Requirement already satisfied: urllib3 in /opt/conda/lib/python3.8/site-packages (from kaggle->opendatasets) (1.26.9)\n","Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.8/site-packages (from python-slugify->kaggle->opendatasets) (1.3)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (3.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (2.0.12)\n","Building wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=2244fefa19538b8d14b36df1525835324e5620bb4dedd3f253555380a07b7c2f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-8azjkl6k/wheels/29/da/11/144cc25aebdaeb4931b231e25fd34b394e6a5725cbb2f50106\n","Successfully built kaggle\n","Installing collected packages: kaggle, opendatasets\n","Successfully installed kaggle-1.5.12 opendatasets-0.1.22\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["!pip install opendatasets"]},{"cell_type":"code","execution_count":null,"id":"c361bc6c","metadata":{"id":"c361bc6c","outputId":"3e995185-dcb6-4f96-e63f-f8a72dc36ada"},"outputs":[{"name":"stdout","output_type":"stream","text":["Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n","Your Kaggle username:"]},{"name":"stdin","output_type":"stream","text":["  wnagesh\n"]},{"name":"stdout","output_type":"stream","text":["Your Kaggle Key:"]},{"name":"stdin","output_type":"stream","text":["  ································\n"]},{"name":"stdout","output_type":"stream","text":["Downloading tuberculosis-tb-chest-xray-dataset.zip to ./tuberculosis-tb-chest-xray-dataset\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 663M/663M [00:20<00:00, 34.6MB/s] \n"]},{"name":"stdout","output_type":"stream","text":["\n"]}],"source":["import opendatasets as od\n","od.download(\"https://www.kaggle.com/datasets/tawsifurrahman/tuberculosis-tb-chest-xray-dataset\")"]},{"cell_type":"code","execution_count":null,"id":"b0370980","metadata":{"id":"b0370980","outputId":"fafa04a7-7cbe-4867-f975-06e71c519895"},"outputs":[{"name":"stdout","output_type":"stream","text":["/workspace/aitrainingandinference\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"id":"ae124544","metadata":{"id":"ae124544"},"outputs":[],"source":["transform = transforms.Compose([transforms.Resize(size=(224,224)), \n","                                transforms.ToTensor(),\n","                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"]},{"cell_type":"code","execution_count":null,"id":"8fa8bc9d","metadata":{"id":"8fa8bc9d"},"outputs":[],"source":["covid_19_dataset = datasets.ImageFolder('./data/database/',transform=transform)"]},{"cell_type":"code","execution_count":null,"id":"acb0d661","metadata":{"id":"acb0d661"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"id":"30e69bc4","metadata":{"id":"30e69bc4"},"outputs":[],"source":["#Hyperparameters\n","config = dict(\n","    saved_path=\"./resnet50_ddp.pt\",\n","    SEED = 42,\n","    lr=0.001, \n","    EPOCHS = 10,\n","    BATCH_SIZE = 32,\n","    IMAGE_SIZE = 224,\n","    TRAIN_VALID_SPLIT = 0.2,\n","    device=device,\n","    pin_memory=False,\n","    num_workers=8,\n","    USE_AMP = False,\n","    channels_last=True,\n","    distributed = False,\n","    world_size=4)"]},{"cell_type":"code","execution_count":null,"id":"4d8e2306","metadata":{"id":"4d8e2306","outputId":"d47573a1-5780-46b7-edd8-c6fec1feec25"},"outputs":[{"data":{"text/html":["Finishing last run (ID:1eubof6j) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">zany-leaf-1</strong>: <a href=\"https://wandb.ai/sateam/Apex-predator/runs/1eubof6j\" target=\"_blank\">https://wandb.ai/sateam/Apex-predator/runs/1eubof6j</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20220722_123832-1eubof6j/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:1eubof6j). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.21"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/workspace/aitrainingandinference/wandb/run-20220722_125604-1urjhyp9</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/sateam/Apex-predator/runs/1urjhyp9\" target=\"_blank\">ethereal-pyramid-2</a></strong> to <a href=\"https://wandb.ai/sateam/Apex-predator\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["#Initiate the Project and Entity\n","wandb.init(project=\"Apex-predator\", config=config,  group=\"DDP\")\n","# access all HPs through wandb.config, so logging matches execution!\n","config = wandb.config"]},{"cell_type":"code","execution_count":null,"id":"609dd138","metadata":{"id":"609dd138"},"outputs":[],"source":["if config.distributed:\n","    # FOR DISTRIBUTED:  Set the device according to local_rank.\n","    torch.cuda.set_device(args.local_rank)       \n","    # FOR DISTRIBUTED:  Initialize the backend.  torch.distributed.launch will provide\n","    # environment variables, and requires that you use init_method=`env://`.\n","    torch.distributed.init_process_group(backend='nccl',\n","                                         init_method='env://')"]},{"cell_type":"code","execution_count":null,"id":"f4cda016","metadata":{"id":"f4cda016"},"outputs":[],"source":["#Pytorch Reproducibility\n","random.seed(config.SEED)\n","np.random.seed(config.SEED)\n","torch.manual_seed(config.SEED)\n","torch.cuda.manual_seed(config.SEED)\n","torch.backends.cudnn.benchmarks = True"]},{"cell_type":"code","execution_count":null,"id":"301cdbad","metadata":{"id":"301cdbad"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"4602f858","metadata":{"id":"4602f858"},"outputs":[],"source":["\n","batch_size = 4\n","# percentage of training set to use as validation\n","test_size = 0.2\n","valid_size = 0.1"]},{"cell_type":"code","execution_count":null,"id":"9dc4869b","metadata":{"id":"9dc4869b"},"outputs":[],"source":["#For test\n","num_data = len(covid_19_dataset)\n","indices_data = list(range(num_data))\n","np.random.shuffle(indices_data)\n","split_tt = int(np.floor(test_size * num_data))\n","train_idx, test_idx = indices_data[split_tt:], indices_data[:split_tt]"]},{"cell_type":"code","execution_count":null,"id":"1b417ad2","metadata":{"id":"1b417ad2"},"outputs":[],"source":["#For Valid\n","num_train = len(train_idx)\n","indices_train = list(range(num_train))\n","np.random.shuffle(indices_train)\n","split_tv = int(np.floor(valid_size * num_train))\n","train_new_idx, valid_idx = indices_train[split_tv:],indices_train[:split_tv]"]},{"cell_type":"code","execution_count":null,"id":"d9050c71","metadata":{"id":"d9050c71"},"outputs":[],"source":["# define samplers for obtaining training and validation batches\n","train_sampler = SubsetRandomSampler(train_new_idx)\n","test_sampler = SubsetRandomSampler(test_idx)\n","valid_sampler = SubsetRandomSampler(valid_idx)"]},{"cell_type":"code","execution_count":null,"id":"e815414d","metadata":{"id":"e815414d"},"outputs":[],"source":["#Dataloader -> Set pin_memory=True and num_workers=8\n","train_dl = DataLoader(covid_19_dataset,\n","                      batch_size=config.BATCH_SIZE,\n","                      shuffle=False,\n","                      num_workers=config.num_workers,\n","                      pin_memory=True,\n","                      sampler=train_sampler)\n","valid_dl = DataLoader(covid_19_dataset,\n","                      batch_size=config.BATCH_SIZE,\n","                      shuffle=False,\n","                      num_workers=config.num_workers,\n","                      pin_memory=True,\n","                     sampler=test_sampler)"]},{"cell_type":"code","execution_count":null,"id":"af84ef3e","metadata":{"id":"af84ef3e"},"outputs":[],"source":["#For Resnet50\n","model = models.resnet50(pretrained=True)\n","#For VGG16\n","#model = models.vgg16(pretrained=True)\n","#or download model from \"https://download.pytorch.org/models/resnet50-0676ba61.pth\""]},{"cell_type":"code","execution_count":null,"id":"a6231fbe","metadata":{"id":"a6231fbe"},"outputs":[],"source":["#model = models.resnet50(pretrained=False)\n","#model.load_state_dict(torch.load(config.pretrained_path))\n","\n","#Modify the classifier for agriculture data\n","num_ftrs = model.fc.in_features\n","model.fc = nn.Sequential(nn.Linear(num_ftrs,512),\n","                        nn.ReLU(),\n","                        nn.Dropout(p=0.3),\n","                        nn.Linear(512,2))\n"]},{"cell_type":"code","execution_count":null,"id":"c5919f35","metadata":{"id":"c5919f35"},"outputs":[],"source":["#Channel Last Optimization in Model\n","if config.channels_last:\n","    model = model.to(config.device, memory_format=torch.channels_last) #CHW --> #HWC\n","else:\n","    model = model.to(config.device)\n","    \n","if config.USE_AMP:\n","    optimizer = FusedAdam(model.parameters(), config.lr)\n","    model,optimizer = amp.initialize(model, optimizer, opt_level=\"O2\") #O0/O1/O2\n","else:\n","    optimizer = optim.Adam(model.parameters(),lr=config.lr)\n","    \n","if config.distributed:\n","    # FOR DISTRIBUTED:  After amp.initialize, wrap the model with\n","    # apex.parallel.DistributedDataParallel.\n","    # model = DistributedDataParallel(model)\n","    # torch.nn.parallel.DistributedDataParallel is also fine, with some added configs:\n","    model = torch.nn.parallel.DistributedDataParallel(model,\n","                                                      device_ids=[args.local_rank],\n","                                                      output_device=args.local_rank)"]},{"cell_type":"code","execution_count":null,"id":"357d50c8","metadata":{"id":"357d50c8"},"outputs":[],"source":["# Loss Function\n","criterion = nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":null,"id":"0284b4e5","metadata":{"id":"0284b4e5"},"outputs":[],"source":["# Each process receives its own batch of \"fake input data\" and \"fake target data.\"\n","# The \"training loop\" in each process just uses this fake batch over and over.\n","# https://github.com/NVIDIA/apex/tree/master/examples/imagenet provides a more realistic\n","# example of distributed data sampling for both training and validation.\n"]},{"cell_type":"code","execution_count":null,"id":"43e124d1","metadata":{"id":"43e124d1"},"outputs":[],"source":["def train_model(model,criterion,optimizer,num_epochs=10):\n","    ############################################################\n","    # tell a to watch what the model gets up to: gradients, weights, and more!\n","    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n","    ############################################################\n","\n","    since = time.time()                                            \n","    batch_ct = 0\n","    example_ct = 0\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","        \n","        #Training\n","        model.train()\n","        for x,y in train_dl: #BS=32 ([BS,3,224,224], [BS,4])            \n","            if config.channels_last:\n","                x = x.to(config.device, non_blocking=True, memory_format=torch.channels_last) #CHW --> #HWC\n","            else:\n","                x = x.to(config.device, non_blocking=True)\n","            y = y.to(config.device, non_blocking=True) #CHW --> #HWC\n","            optimizer.zero_grad()\n","            train_logits = model(x) #Input = [BS,3,224,224] (Image) -- Model --> [BS,4] (Output Scores)\n","            _, train_preds = torch.max(train_logits, 1)\n","            train_loss = criterion(train_logits,y)\n","            \n","            ########################################################################\n","            if config.USE_AMP:\n","                with amp.scale_loss(train_loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","                    loss=scaled_loss\n","            else:\n","                train_loss.backward() # Backpropagation this is where your W_gradient\n","                loss=train_loss\n","\n","            optimizer.step() # W_new = W_old - LR * W_gradient \n","            example_ct += len(x) \n","            batch_ct += 1\n","            # Report metrics every 25th batch\n","            if ((batch_ct + 1) % 25) == 0:\n","                train_log(loss, example_ct, epoch)\n","        \n","        \n","        #validation\n","        model.eval()\n","        running_loss = 0.0\n","        running_corrects = 0\n","        total = 0\n","        with torch.no_grad():\n","            for x,y in valid_dl:\n","                if config.channels_last:\n","                    x = x.to(config.device, non_blocking=True, memory_format=torch.channels_last) #CHW --> #HWC\n","                else:\n","                    x = x.to(config.device, non_blocking=True)\n","                y = y.to(config.device, non_blocking=True) #CHW --> #HWC\n","                valid_logits = model(x)\n","                _, valid_preds = torch.max(valid_logits, 1)\n","                valid_loss = criterion(valid_logits,y)\n","                running_loss += valid_loss.item() * x.size(0)\n","                running_corrects += torch.sum(valid_preds == y.data)\n","                total += y.size(0)\n","                wandb.log({\"test_accuracy\": running_corrects / total})\n","            \n","        epoch_loss = running_loss / len(valid_dl)\n","        epoch_acc = running_corrects.double() / len(valid_dl)\n","        print(\"Validation Loss is {}\".format(epoch_loss))\n","        print(\"Validation Accuracy is {}\".format(epoch_acc.cpu()))\n","\n","            \n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    \n","    torch.save(model.state_dict(), config.saved_path)"]},{"cell_type":"code","execution_count":null,"id":"dfe4ccb2","metadata":{"id":"dfe4ccb2"},"outputs":[],"source":["def train_log(loss, example_ct, epoch):\n","    loss = float(loss)\n","    # where the magic happens\n","    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n","    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")\n","    "]},{"cell_type":"code","execution_count":null,"id":"83c4dda5","metadata":{"id":"83c4dda5","outputId":"43dde0db-e759-4e20-b9ce-94f4c564fb1b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0/9\n","----------\n","Loss after 00768 examples: 0.000\n","Loss after 01568 examples: 0.000\n","Loss after 02368 examples: 0.000\n","Validation Loss is 324.08106570773657\n","Validation Accuracy is 26.037037037037035\n","Epoch 1/9\n","----------\n","Loss after 03152 examples: 0.000\n","Loss after 03952 examples: 0.000\n","Loss after 04752 examples: 0.000\n","Loss after 05552 examples: 0.000\n","Validation Loss is 325.24939106128835\n","Validation Accuracy is 26.037037037037035\n","Epoch 2/9\n","----------\n","Loss after 06336 examples: 0.000\n","Loss after 07136 examples: 0.000\n","Loss after 07936 examples: 0.000\n","Loss after 08736 examples: 0.000\n","Validation Loss is 324.7661372997143\n","Validation Accuracy is 26.037037037037035\n","Epoch 3/9\n","----------\n","Loss after 09520 examples: 0.000\n","Loss after 10320 examples: 0.000\n","Loss after 11120 examples: 0.000\n","Loss after 11920 examples: 0.000\n","Validation Loss is 322.6980387369792\n","Validation Accuracy is 26.037037037037035\n","Epoch 4/9\n","----------\n","Loss after 12704 examples: 0.000\n","Loss after 13504 examples: 0.000\n","Loss after 14304 examples: 0.000\n","Loss after 15104 examples: 0.000\n","Validation Loss is 320.76440684000653\n","Validation Accuracy is 26.037037037037035\n","Epoch 5/9\n","----------\n","Loss after 15888 examples: 0.000\n","Loss after 16688 examples: 0.000\n","Loss after 17488 examples: 0.000\n","Validation Loss is 321.5751224093967\n","Validation Accuracy is 26.037037037037035\n","Epoch 6/9\n","----------\n","Loss after 18272 examples: 0.000\n","Loss after 19072 examples: 0.000\n","Loss after 19872 examples: 0.000\n","Loss after 20672 examples: 0.000\n","Validation Loss is 321.49356064973057\n","Validation Accuracy is 26.037037037037035\n","Epoch 7/9\n","----------\n","Loss after 22256 examples: 0.000\n","Loss after 23056 examples: 0.000\n","Loss after 23856 examples: 0.000\n","Validation Loss is 324.57124667697485\n","Validation Accuracy is 26.037037037037035\n","Epoch 8/9\n","----------\n","Loss after 24640 examples: 0.000\n","Loss after 25440 examples: 0.000\n","Loss after 26240 examples: 0.000\n","Loss after 27040 examples: 0.000\n","Validation Loss is 324.05057370221175\n","Validation Accuracy is 26.037037037037035\n","Epoch 9/9\n","----------\n","Loss after 27824 examples: 0.000\n","Loss after 28624 examples: 0.000\n","Loss after 29424 examples: 0.000\n","Loss after 30224 examples: 0.000\n","Validation Loss is 322.8593266805013\n","Validation Accuracy is 26.037037037037035\n","Training complete in 3m 53s\n"]}],"source":["train_model(model, criterion, optimizer, num_epochs=config.EPOCHS)"]},{"cell_type":"code","execution_count":null,"id":"83bf776e","metadata":{"id":"83bf776e"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"colab":{"name":"train.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}